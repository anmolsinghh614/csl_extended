{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjM7L9NPvCH9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGy9NWm_Po8a",
        "outputId": "9b5f4ef0-8550-48f2-cb1a-7aa722c4c4b0"
      },
      "outputs": [],
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k9Fz9dLvm7M"
      },
      "source": [
        "DATASET   CIFAR 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXIIlGYOJCZW"
      },
      "outputs": [],
      "source": [
        "imbalance_factor = 100    #100, 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JcWBgAF_7Qxm",
        "outputId": "50940383-5586-461c-9dde-1d2aee517c93"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optimized Data Preprocessing\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),  # Combined resizing and cropping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize directly to target size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_set = datasets.CIFAR100(root='./datasets', train=True, download=True, transform=transform_train)\n",
        "test_set = datasets.CIFAR100(root='./datasets', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Define imbalance factor and number of classes\n",
        "num_classes = 100\n",
        "\n",
        "# Calculate distribution\n",
        "mu = (1 / imbalance_factor) ** (1 / (num_classes - 1))\n",
        "\n",
        "def calculate_class_distribution(dataset, num_classes, mu):\n",
        "    original_counts = np.bincount([label for _, label in dataset], minlength=num_classes)\n",
        "    max_samples = max(original_counts)\n",
        "    target_counts = [int(max_samples * (mu ** i)) for i in range(num_classes)]\n",
        "    return target_counts\n",
        "\n",
        "class_distribution_train = calculate_class_distribution(train_set, num_classes, mu)\n",
        "\n",
        "# Apply class distribution to training set\n",
        "def apply_class_distribution(dataset, target_counts):\n",
        "    class_counts = np.zeros(len(target_counts), dtype=int)\n",
        "    filtered_indices = []\n",
        "\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        if class_counts[label] < target_counts[label]:\n",
        "            filtered_indices.append(idx)\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    return torch.utils.data.Subset(dataset, filtered_indices)\n",
        "\n",
        "train_set = apply_class_distribution(train_set, class_distribution_train)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_size = int(0.8 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "# Prepare the training and testing datasets\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Calculate class distributions\n",
        "train_class_counts = np.bincount([train_set[i][1] for i in range(len(train_set))], minlength=num_classes)\n",
        "val_class_counts = np.bincount([val_set[i][1] for i in range(len(val_set))], minlength=num_classes)\n",
        "test_class_counts = np.bincount([label for _, label in test_set], minlength=num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w9BEVzC_Gtt",
        "outputId": "d30f5db9-c95f-4c92-c5af-e72f70c22690"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optimized Data Preprocessing\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),  # Combined resizing and cropping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize directly to target size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_set = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_set = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Define imbalance factor and number of classes\n",
        "num_classes = 100\n",
        "\n",
        "# Assuming imbalance_factor is defined somewhere\n",
        "mu = (1 / imbalance_factor) ** (1 / (num_classes - 1))\n",
        "\n",
        "def calculate_class_distribution(dataset, num_classes, mu):\n",
        "    original_counts = np.bincount([label for _, label in dataset], minlength=num_classes)\n",
        "    max_samples = max(original_counts)\n",
        "    target_counts = [int(max_samples * (mu ** i)) for i in range(num_classes)]\n",
        "    return target_counts\n",
        "\n",
        "class_distribution_train = calculate_class_distribution(train_set, num_classes, mu)\n",
        "\n",
        "# Apply class distribution to training set\n",
        "def apply_class_distribution(dataset, target_counts):\n",
        "    class_counts = np.zeros(len(target_counts), dtype=int)\n",
        "    filtered_indices = []\n",
        "\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        if class_counts[label] < target_counts[label]:\n",
        "            filtered_indices.append(idx)\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    return torch.utils.data.Subset(dataset, filtered_indices)\n",
        "\n",
        "train_set = apply_class_distribution(train_set, class_distribution_train)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_size = int(0.8 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "# Prepare the training and testing datasets\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Calculate class distributions\n",
        "train_class_counts = np.bincount([train_set[i][1] for i in range(len(train_set))], minlength=num_classes)\n",
        "\n",
        "# Categorize classes into many, medium, and few\n",
        "many_classes = [i for i, count in enumerate(train_class_counts) if count > 250]\n",
        "medium_classes = [i for i, count in enumerate(train_class_counts) if 75 <= count <= 250]\n",
        "few_classes = [i for i, count in enumerate(train_class_counts) if count < 75]\n",
        "\n",
        "# Print class categorization\n",
        "print(\"Many classes (more than 250 samples):\", many_classes)\n",
        "print(\"Medium classes (between 75 and 250 samples):\", medium_classes)\n",
        "print(\"Few classes (less than 75 samples):\", few_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu-21ac5zxcx"
      },
      "source": [
        "#Resnet-32 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LubC-XrvvkBM"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def create_resnet32():\n",
        "    return ResNet32(BasicBlock, [5, 5, 5])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_resnet32().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHjhTPUTbUId"
      },
      "source": [
        "LOSS TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcbSgA0WKUBi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class AdditionalTermLayer(nn.Module):\n",
        "    def __init__(self, target_class_index, num_classes):\n",
        "        super(AdditionalTermLayer, self).__init__()\n",
        "        self.target_class_index = target_class_index\n",
        "        self.num_classes = num_classes\n",
        "        self.previous_epoch_class_predictions = None\n",
        "        self.feature_storage = {i: [] for i in range(num_classes)}\n",
        "        self.gamma_values = []\n",
        "        self.class_predictions_history = []\n",
        "        self.semantic_scales_history = []\n",
        "        self.entropies = {i: [] for i in range(num_classes)}  # Track entropy for each class\n",
        "\n",
        "    def compute_entropy(self, class_predictions, num_samples):\n",
        "        \"\"\"\n",
        "        Compute entropy for class i based on its predictions.\n",
        "        \"\"\"\n",
        "        probabilities = class_predictions.float() / num_samples\n",
        "        non_zero_probs = probabilities[probabilities > 0]\n",
        "        entropy = -torch.sum(non_zero_probs * torch.log(non_zero_probs + 1e-6))  # Add small value to avoid log(0)\n",
        "        return entropy.item()\n",
        "\n",
        "    def forward(self, inputs, true_labels, epoch):\n",
        "        inputs = torch.nan_to_num(inputs)  # Replace NaNs with zero\n",
        "        additional_term = 0.0\n",
        "\n",
        "        class_predictions = torch.argmax(inputs, dim=-1)\n",
        "\n",
        "        # Store the current batch's features\n",
        "        for i in range(self.num_classes):\n",
        "            class_indices = (true_labels == i).nonzero(as_tuple=True)[0]\n",
        "            if class_indices.size(0) > 0:  # Ensure class_indices is not empty\n",
        "                self.feature_storage[i].extend(inputs[class_indices].detach().cpu().numpy())\n",
        "\n",
        "        # Calculate the semantic scale for each class\n",
        "        semantic_scales = []\n",
        "        for features in self.feature_storage.values():\n",
        "            if len(features) > 0:\n",
        "                features = np.array(features)\n",
        "                avg_magnitude = np.mean(np.linalg.norm(features, axis=1))\n",
        "                semantic_scale = avg_magnitude ** 2\n",
        "                semantic_scales.append(semantic_scale)\n",
        "            else:\n",
        "                semantic_scales.append(0.0)\n",
        "\n",
        "        # Store the semantic scales for all epochs\n",
        "        self.semantic_scales_history.append(semantic_scales.copy())\n",
        "\n",
        "        # Calculate class entropies\n",
        "        class_entropies = []\n",
        "        num_samples = len(true_labels)\n",
        "        for i in range(self.num_classes):\n",
        "            class_indices = (true_labels == i).nonzero(as_tuple=True)[0]\n",
        "            class_predictions_i = (class_predictions == i).float()\n",
        "            entropy = self.compute_entropy(class_predictions_i, num_samples)\n",
        "            self.entropies[i].append(entropy)\n",
        "            class_entropies.append(entropy)\n",
        "\n",
        "        # Calculate gamma values using the updated formula\n",
        "        max_semantic_scale = max(semantic_scales) + 1e-6\n",
        "        dynamic_gammas = [\n",
        "            scale / (1e-6 + max_semantic_scale * entropy)\n",
        "            for scale, entropy in zip(semantic_scales, class_entropies)\n",
        "        ]\n",
        "\n",
        "        # Store gamma values for all epochs\n",
        "        self.gamma_values.append(dynamic_gammas.copy())\n",
        "\n",
        "        # Calculate the number of predictions for each class\n",
        "        current_epoch_class_predictions = torch.tensor([\n",
        "            torch.sum((class_predictions == i).float()).item() for i in range(self.num_classes)\n",
        "        ])\n",
        "\n",
        "        # Store class predictions for all epochs\n",
        "        self.class_predictions_history.append(current_epoch_class_predictions.tolist())\n",
        "\n",
        "        # Compute the additional term\n",
        "        for i, gamma in enumerate(dynamic_gammas):\n",
        "            class_i_predictions = current_epoch_class_predictions[i]\n",
        "            if i in self.target_class_index:\n",
        "                if self.previous_epoch_class_predictions is not None:\n",
        "                    previous_class_i_predictions = self.previous_epoch_class_predictions[i]\n",
        "                    reinforcement_term = torch.tensor(0.0)\n",
        "                    if class_i_predictions > previous_class_i_predictions:\n",
        "                        reinforcement_term = -2.0\n",
        "                    elif class_i_predictions < previous_class_i_predictions:\n",
        "                        reinforcement_term = 2.0\n",
        "                else:\n",
        "                    reinforcement_term = torch.tensor(0.0)\n",
        "            else:\n",
        "                reinforcement_term = torch.tensor(0.0)\n",
        "\n",
        "            term = (gamma * class_i_predictions + reinforcement_term) ** 2\n",
        "            denom = torch.sum((inputs - F.one_hot(torch.tensor(i), num_classes=self.num_classes).float().to(inputs.device)) ** 2) + 1e-6  # Add small value to avoid division by zero\n",
        "            additional_term += term / denom\n",
        "\n",
        "        # Normalize the additional term\n",
        "        additional_term /= self.num_classes\n",
        "\n",
        "        self.previous_epoch_class_predictions = current_epoch_class_predictions\n",
        "\n",
        "        return additional_term\n",
        "\n",
        "class CustomLossWithL2AndAdditionalTerm(nn.Module):\n",
        "    def __init__(self, target_class_index, num_classes):\n",
        "        super(CustomLossWithL2AndAdditionalTerm, self).__init__()\n",
        "        self.additional_term_layer = AdditionalTermLayer(target_class_index, num_classes)\n",
        "\n",
        "    def forward(self, y_true, y_pred, epoch):\n",
        "        y_true_one_hot = F.one_hot(y_true.squeeze().long(), num_classes=y_pred.size(-1)).float()\n",
        "        cross_entropy_loss = F.cross_entropy(y_pred, y_true)\n",
        "\n",
        "        additional_term = self.additional_term_layer(y_pred, y_true, epoch)\n",
        "        total_loss = cross_entropy_loss + additional_term\n",
        "\n",
        "        # Debugging statements\n",
        "        assert not torch.isnan(cross_entropy_loss).any(), \"cross_entropy_loss has NaNs\"\n",
        "        assert not torch.isnan(additional_term).any(), \"additional_term_layer has NaNs\"\n",
        "        assert not torch.isnan(total_loss).any(), \"total_loss has NaNs\"\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plu7G_44MW_p"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgbfmyibLtr-",
        "outputId": "8f7324fa-17ae-42e4-b046-01efd8828736"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "target_class_index = list(range(0, 100))\n",
        "initial_learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "num_classes = 100  # Assuming 100 classes\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=2e-4)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.99 ** epoch)\n",
        "\n",
        "# Loss function\n",
        "criterion = CustomLossWithL2AndAdditionalTerm(target_class_index, num_classes).to(device)\n",
        "\n",
        "# Add this list to store validation accuracy\n",
        "val_accuracies = []\n",
        "\n",
        "# Initialize class-wise correct and total counters\n",
        "train_class_correct = np.zeros(num_classes)\n",
        "train_class_total = np.zeros(num_classes)\n",
        "val_class_correct = np.zeros(num_classes)\n",
        "val_class_total = np.zeros(num_classes)\n",
        "\n",
        "# Define thresholds for many, medium, and few\n",
        "many_threshold = 250\n",
        "medium_min_threshold = 75\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(labels, outputs, epoch)  # Pass the current epoch to the loss function\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Update class-wise statistics\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "            train_class_total[label] += 1\n",
        "            if prediction == label:\n",
        "                train_class_correct[label] += 1\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = 100. * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(labels, outputs, epoch)  # Pass the current epoch to the loss function\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Update class-wise statistics for validation\n",
        "            for label, prediction in zip(labels, predicted):\n",
        "                val_class_total[label] += 1\n",
        "                if prediction == label:\n",
        "                    val_class_correct[label] += 1\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = 100. * correct / total\n",
        "\n",
        "    # Store validation accuracy for plotting later\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Learning rate update\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "# Calculate overall accuracies for many, medium, and few classes\n",
        "train_many_correct = train_class_correct[train_class_total > many_threshold].sum()\n",
        "train_many_total = train_class_total[train_class_total > many_threshold].sum()\n",
        "train_medium_correct = train_class_correct[(train_class_total <= many_threshold) & (train_class_total >= medium_min_threshold)].sum()\n",
        "train_medium_total = train_class_total[(train_class_total <= many_threshold) & (train_class_total >= medium_min_threshold)].sum()\n",
        "train_few_correct = train_class_correct[train_class_total < medium_min_threshold].sum()\n",
        "train_few_total = train_class_total[train_class_total < medium_min_threshold].sum()\n",
        "\n",
        "# Print overall and category-wise accuracy\n",
        "print(\"\\nOverall Accuracy: {:.2f}%\".format(100. * correct / total))\n",
        "print(\"Many Classes Accuracy: {:.2f}%\".format(100. * train_many_correct / train_many_total if train_many_total > 0 else 0))\n",
        "print(\"Medium Classes Accuracy: {:.2f}%\".format(100. * train_medium_correct / train_medium_total if train_medium_total > 0 else 0))\n",
        "print(\"Few Classes Accuracy: {:.2f}%\".format(100. * train_few_correct / train_few_total if train_few_total > 0 else 0))\n",
        "\n",
        "# Function to plot validation accuracy\n",
        "def plot_validation_accuracy(val_accuracies):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Accuracy (%)')\n",
        "    plt.title('Validation Accuracy Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the validation accuracy\n",
        "plot_validation_accuracy(val_accuracies)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
